## Heading to the Analytical Directions: Extract, Transform, Load:

#### Problem Statement:

A Music streaming platform Sparkify does have loads of user session data generated by their servers in **.json** format. 
To get the valueable insights out of these user data and turn it into a valuable story, the data needs to analyzed by 
their Data Scientists. We as Data Engineers, our action is to create an pipeline that with automation extracts the data 
from the raw format then processes it and transforms, next up which can be sent to the analysts. 

There are a total of two domains of data is stored on their datastore `logs_data` which contains the user activity as it 
contains all the users' listening habits and metadata of the session. `songs_data` contains every details about each song 
available in the application both are stored in **JSON** format. 

### Explaining the Files:

In the **/data** folder we have got another two directories which are **song_data** and the **log_data**. 

##### song_data: 
The first dataset contained in the **song_data** folder is a subset of real data from the Million Song Dataset. Each file is 
in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three 
letters of each song's track ID. For example, here are filepaths to two files in this dataset which conforms to the directory 
structure of these dataset.
```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```
An instance of the data file inside these datasets are: 
```
{"num_songs": 1, "artist_id": "ARI2JSK1187FB496EF", "artist_latitude": 51.50632, "artist_longitude": -0.12714, "artist_location": "London, England", "artist_name": "Nick Ingman;Gavyn Wright", "song_id": "SODUJBS12A8C132150", "title": "Wessex Loses a Bride", "duration": 111.62077, "year": 0}
```

##### log_data:
The second dataset contained in the **logs_data** is of log files in JSON format generated by an [event simulator]('https://github.com/Interana/eventsim') based on the songs in the dataset above. These simulate activity logs from 
a music streaming app based on specified configurations.

The log files in the dataset we are working with are partitioned by year and month. For example, filepaths to two 
files in this dataset.

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```
An instance of the data file inside these datasets are:
```
{"artist":null,"auth":"Logged In","firstName":"Walter","gender":"M","itemInSession":0,"lastName":"Frye","length":null,"level":"free","location":"San Francisco-Oakland-Hayward, CA","method":"GET","page":"Home","registration":1540919166796.0,"sessionId":38,"song":null,"status":200,"ts":1541105830796,"userAgent":"\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"","userId":"39"}
```

#### Solution Statement:

To make the most out of the data we try to use the power of **Data Modeling**. For this reason we use relation database 
**PostgreSql** with validated definite schema to split up the data into logically different tables using the Star Schema 
for Datamarts. The schema for this database is shown below:

       `songsplay` -> Fact Table       [Contains Information about the user activities]
       `artists`   -> Dimension Table  [Contains Information about Each artists' Metadata]
       `users`     -> Dimension Table  [Contains Information about Each Users' Metadata]
       `songs`     -> Dimension Table  [Contains Infomation about Each Song's Metadata]
       `time`      -> Dimension Table  [Contains Information about Session Timings]

We create an ETL Pipeline using SQL to create PostgreSql Tables inside a PostgreSql Relational Database model and load the
**JSON** data using **Pandas DataFrame** and we are inserting the data using **.values()**. We have split-up the data and 
insert the data into the **Fact Table** and the **Dimension Tables**. 

### Setting Up the Project:

To set-up the project we need issue the following commands:

```
python sql_queries.py
python create_tables.py
python etl.py
```

